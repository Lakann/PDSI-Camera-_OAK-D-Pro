# TUTORIAIS

Instalando as depend√™ncias da c√¢mera
   Link para instalar as dependencias da c√¢mera (abra em uma nova guia): <a href="https://docs.luxonis.com/software/depthai/manual-install/#Manual%20DepthAI%20installation-Installing%20dependencies">Clique aqui</a>
 


<!-- Pr√≥ximo t√≥pico -->


# üìö Tutoriais

Ap√≥s realizar o download e a instala√ß√£o das depend√™ncias da c√¢mera, assim como a instala√ß√£o correta da OpenCV, j√° √© poss√≠vel executar alguns exemplos pr√°ticos. Esses exemplos podem ser feitos com a c√¢mera OAK-D ou, caso voc√™ n√£o possua a c√¢mera no momento, podem ser adaptados para a webcam do notebook ou PC.
  
Para mais detalhes, esses e mais exemplos podem ser encontrados no site oficial da Luxonis em <a href="https://docs.luxonis.com/">Docs Luxonis</a>.

<details>
summary>## üëã Hello World</summary>

Esse exemplo foi retirado do site da Luxonis e pode ser executado tanto na c√¢mera OAK-D quanto na c√¢mera do seu notebook/PC.

‚ö†Ô∏è Aten√ß√£o: antes de rodar o c√≥digo, certifique-se de selecionar o interpretador Python correto ‚Äî aquele em que voc√™ instalou o OpenCV, o DepthAI e as demais depend√™ncias. Recomenda-se que essas bibliotecas sejam instaladas e configuradas dentro de um ambiente virtual (venv) para garantir isolamento e evitar conflitos com outros projetos.
    
Vamos mergulhar nos conceitos b√°sicos usando um exemplo. Vamos criar uma aplica√ß√£o simples que executa uma rede neural de detec√ß√£o de objetos e transmite v√≠deo em cores com        as detec√ß√µes da rede neural visualizadas. Usaremos a API Python do DepthAI para criar a aplica√ß√£o.

O primeiro n√≥ que adicionaremos √© o **ColorCamera**. Esse n√≥ selecionar√° automaticamente a c√¢mera central (que, na maioria dos dispositivos, √© a c√¢mera de cor) e fornecer√° o fluxo de v√≠deo para o pr√≥ximo n√≥ no pipeline.
Usaremos a sa√≠da **preview**, redimensionada para 300x300, de forma a se ajustar ao tamanho de entrada do **mobilenet-ssd** (que definiremos mais adiante).
    
### C√¢mera

```
# Primeiro, importamos todos os m√≥dulos necess√°rios
from pathlib import Path

import blobconverter
import cv2
import depthai
import numpy as np


pipeline = depthai.Pipeline()

# Primeiro, queremos a c√¢mera de cor como sa√≠da
cam_rgb = pipeline.createColorCamera()
cam_rgb.setPreviewSize(300, 300)  # 300x300 ser√° o tamanho do frame de pr√©-visualiza√ß√£o, dispon√≠vel como sa√≠da 'preview' do n√≥
cam_rgb.setInterleaved(False)

detection_nn = pipeline.createMobileNetDetectionNetwork()

# O blob √© o arquivo da Rede Neural, compilado para MyriadX. Ele cont√©m tanto a defini√ß√£o quanto os pesos do modelo
# Estamos usando a ferramenta blobconverter para obter automaticamente o blob do MobileNetSSD a partir do OpenVINO Model Zoo

detection_nn.setBlobPath(blobconverter.from_zoo(name='mobilenet-ssd', shaves=6))

# Em seguida, filtramos as detec√ß√µes que est√£o abaixo de um limite de confian√ßa. A confian√ßa pode estar entre <0..1>
detection_nn.setConfidenceThreshold(0.5)

# XLinkOut √© uma "sa√≠da" do dispositivo. Qualquer dado que voc√™ queira transferir para o host precisa ser enviado via XLink
xout_rgb = pipeline.createXLinkOut()
xout_rgb.setStreamName("rgb")

xout_nn = pipeline.createXLinkOut()
xout_nn.setStreamName("nn")

cam_rgb.preview.link(xout_rgb.input)
cam_rgb.preview.link(detection_nn.input)
detection_nn.out.link(xout_nn.input)

# O pipeline agora est√° finalizado e precisamos encontrar um dispositivo dispon√≠vel para execut√°-lo
# Estamos usando um context manager aqui, que ir√° liberar o dispositivo ap√≥s o uso
with depthai.Device(pipeline) as device:
    # A partir deste ponto, o dispositivo estar√° em modo "executando" e come√ßar√° a enviar dados via XLink

    # Para consumir os resultados do dispositivo, obtemos duas filas de sa√≠da com os nomes de stream definidos anteriormente
    q_rgb = device.getOutputQueue("rgb")
    q_nn = device.getOutputQueue("nn")

    # Aqui alguns valores padr√£o s√£o definidos. Frame ser√° uma imagem do stream "rgb" e detections conter√° os resultados da rede neural
    frame = None
    detections = []

    # Como as detec√ß√µes retornadas pela rede neural possuem valores no intervalo <0..1>, 
    # eles precisam ser multiplicados pela largura/altura do frame para obter a posi√ß√£o real da caixa delimitadora na imagem
    def frameNorm(frame, bbox):
        normVals = np.full(len(bbox), frame.shape[0])
        normVals[::2] = frame.shape[1]
        return (np.clip(np.array(bbox), 0, 1) * normVals).astype(int)


    while True:
        # Tentamos buscar os dados das filas nn/rgb. tryGet retorna ou o pacote de dados ou None se n√£o houver nada
        in_rgb = q_rgb.tryGet()
        in_nn = q_nn.tryGet()

        if in_rgb is not None:
            # Se o pacote da c√¢mera RGB estiver presente, recuperamos o frame no formato OpenCV usando getCvFrame
            frame = in_rgb.getCvFrame()

        if in_nn is not None:
            # Quando os dados da rede neural s√£o recebidos, pegamos o array de detec√ß√µes que cont√©m os resultados do mobilenet-ssd
            detections = in_nn.detections


        if frame is not None:
            for detection in detections:
                # Para cada caixa delimitadora, primeiro normalizamos para corresponder ao tamanho do frame
                bbox = frameNorm(frame, (detection.xmin, detection.ymin, detection.xmax, detection.ymax))

                # E ent√£o desenhamos um ret√¢ngulo no frame para mostrar o resultado
                cv2.rectangle(frame, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (255, 0, 0), 2)

            # Ap√≥s todo o desenho estar conclu√≠do, mostramos o frame na tela
            cv2.imshow("preview", frame)

        # A qualquer momento, voc√™ pode pressionar "q" para sair do loop principal, encerrando o programa
        if cv2.waitKey(1) == ord('q'):
            break

```







### Notebook

```
import cv2
import numpy as np

# ----------------- CONFIGURA√á√ÉO -----------------
# Confian√ßa m√≠nima para exibir detec√ß√µes
CONFIDENCE_THRESHOLD = 0.5

# Caminhos dos arquivos do modelo MobileNet-SSD
# Voc√™ precisa baixar esses arquivos:
# 1. prototxt: https://github.com/chuanqi305/MobileNet-SSD/blob/master/deploy.prototxt
# 2. caffemodel: https://github.com/chuanqi305/MobileNet-SSD/blob/master/MobileNetSSD_deploy.caffemodel
MODEL_PROTOTXT = "mobilenet_ssd.prototxt"
MODEL_WEIGHTS = "mobilenet_ssd.caffemodel"

# ----------------- INICIALIZA√á√ÉO -----------------
cap = cv2.VideoCapture(0)  # 0 = webcam interna
if not cap.isOpened():
    raise RuntimeError("N√£o foi poss√≠vel abrir a webcam do notebook")

# Carrega o modelo MobileNet-SSD
net = cv2.dnn.readNetFromCaffe(MODEL_PROTOTXT, MODEL_WEIGHTS)

# Lista de classes que o MobileNet-SSD detecta
CLASSES = ["background", "aeroplane", "bicycle", "bird", "boat",
           "bottle", "bus", "car", "cat", "chair", "cow", "diningtable",
           "dog", "horse", "motorbike", "person", "pottedplant",
           "sheep", "sofa", "train", "tvmonitor"]

# ----------------- LOOP PRINCIPAL -----------------
while True:
    ret, frame = cap.read()
    if not ret:
        break

    # Redimensiona e pr√©-processa o frame
    blob = cv2.dnn.blobFromImage(frame, 0.007843, (300, 300), 127.5)
    net.setInput(blob)
    detections = net.forward()

    # Percorre todas as detec√ß√µes
    for i in range(detections.shape[2]):
        confidence = detections[0, 0, i, 2]
        if confidence > CONFIDENCE_THRESHOLD:
            idx = int(detections[0, 0, i, 1])
            box = detections[0, 0, i, 3:7] * np.array([frame.shape[1], frame.shape[0],
                                                       frame.shape[1], frame.shape[0]])
            (startX, startY, endX, endY) = box.astype("int")

            # Desenha o ret√¢ngulo e a classe detectada
            label = f"{CLASSES[idx]}: {confidence*100:.1f}%"
            cv2.rectangle(frame, (startX, startY), (endX, endY), (255, 0, 0), 2)
            y = startY - 15 if startY - 15 > 15 else startY + 15
            cv2.putText(frame, label, (startX, y),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)

    # Mostra o frame com as detec√ß√µes
    cv2.imshow("Webcam Preview", frame)

    # Sai ao pressionar 'q'
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()
```

## Para fechar as janelas basta apertar "q"
</details>
